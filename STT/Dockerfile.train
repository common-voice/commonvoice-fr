# >> START Build STT

# Need devel version cause we need /usr/include/cudnn.h
FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 as stt-build

ARG stt_repo=wasertech/STT
#coqui-ai/STT
ARG stt_branch=aee6956a9bee8bda89dba35378a39e6fe2963c62
#8aa2b13e987e2f397b6b6b3ca1ae81c7ebac72d1
ARG stt_sha1=aee6956a9bee8bda89dba35378a39e6fe2963c62
#8aa2b13e987e2f397b6b6b3ca1ae81c7ebac72d1

# Get basic packages
RUN apt-get update && apt-get install -y --no-install-recommends \
        apt-utils \
        bash-completion \
        build-essential \
        ca-certificates \
        cmake \
        curl \
        g++ \
        gcc \
        git \
        libbz2-dev \
        libboost-all-dev \
        libgsm1-dev \
        libltdl-dev \
        liblzma-dev \
        libmagic-dev \
        libpng-dev \
        libsox-fmt-mp3 \
        libsox-dev \
        locales \
        openjdk-8-jdk \
        pkg-config \
        python3 \
        python3-dev \
        python3-pip \
        python3-wheel \
        python3-numpy \
        sox \
        unzip \
        wget \
        zlib1g-dev

RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1

# Install Bazel
RUN curl -LO "https://github.com/bazelbuild/bazel/releases/download/3.1.0/bazel_3.1.0-linux-x86_64.deb"
RUN dpkg -i bazel_*.deb

# Try and free some space
RUN rm -rf /var/lib/apt/lists/*

# << END Install base software

# >> START Configure Tensorflow Build

# GPU Environment Setup
ENV TF_NEED_ROCM 0
ENV TF_NEED_OPENCL_SYCL 0
ENV TF_NEED_OPENCL 0
ENV TF_NEED_CUDA 1
ENV TF_CUDA_PATHS "/usr,/usr/local/cuda-10.1,/usr/lib/x86_64-linux-gnu/"
ENV TF_CUDA_VERSION 10.1
ENV TF_CUDNN_VERSION 7.6
ENV TF_CUDA_COMPUTE_CAPABILITIES 6.0
ENV TF_NCCL_VERSION 2.8

# Common Environment Setup
ENV TF_BUILD_CONTAINER_TYPE GPU
ENV TF_BUILD_OPTIONS OPT
ENV TF_BUILD_DISABLE_GCP 1
ENV TF_BUILD_ENABLE_XLA 0
ENV TF_BUILD_PYTHON_VERSION PYTHON3
ENV TF_BUILD_IS_OPT OPT
ENV TF_BUILD_IS_PIP PIP

# Other Parameters
ENV CC_OPT_FLAGS -mavx -mavx2 -msse4.1 -msse4.2 -mfma
ENV TF_NEED_GCP 0
ENV TF_NEED_HDFS 0
ENV TF_NEED_JEMALLOC 1
ENV TF_NEED_OPENCL 0
ENV TF_CUDA_CLANG 0
ENV TF_NEED_MKL 0
ENV TF_ENABLE_XLA 0
ENV TF_NEED_AWS 0
ENV TF_NEED_KAFKA 0
ENV TF_NEED_NGRAPH 0
ENV TF_DOWNLOAD_CLANG 0
ENV TF_NEED_TENSORRT 0
ENV TF_NEED_GDR 0
ENV TF_NEED_VERBS 0
ENV TF_NEED_OPENCL_SYCL 0

ENV PYTHON_BIN_PATH /usr/bin/python3.6
ENV PYTHON_LIB_PATH /usr/local/lib/python3.6/dist-packages

# << END Configure Tensorflow Build

# >> START Configure Bazel

# Running bazel inside a `docker build` command causes trouble, cf:
#   https://github.com/bazelbuild/bazel/issues/134
# The easiest solution is to set up a bazelrc file forcing --batch.
RUN echo "startup --batch" >>/etc/bazel.bazelrc
# Similarly, we need to workaround sandboxing issues:
#   https://github.com/bazelbuild/bazel/issues/418
RUN echo "build --spawn_strategy=standalone --genrule_strategy=standalone" \
    >>/etc/bazel.bazelrc

# << END Configure Bazel

WORKDIR /
RUN git clone https://github.com/${stt_repo} STT && cd /STT && git checkout ${stt_branch} && git submodule sync && git submodule update --init tensorflow

# >> START Build and bind

WORKDIR /STT/tensorflow

# Fix for not found script https://github.com/tensorflow/tensorflow/issues/471
RUN /STT/tensorflow/configure

# Using CPU optimizations:
# -mtune=generic -march=x86-64 -msse -msse2 -msse3 -msse4.1 -msse4.2 -mavx.
# Adding --config=cuda flag to build using CUDA.

# passing LD_LIBRARY_PATH is required cause Bazel doesn't pickup it from environment

# Build STT
RUN bazel build \
	--verbose_failures \
	--workspace_status_command="bash native_client/bazel_workspace_status_cmd.sh" \
    --config=cuda \
    -c opt \
	--copt=-mtune=generic \
	--copt=-march=x86-64 \
	--copt=-msse \
	--copt=-msse2 \
	--copt=-msse3 \
	--copt=-msse4.1 \
	--copt=-msse4.2 \
	--copt=-mavx \
	--config=noaws \
	--config=nogcp \
	--config=nohdfs \
	--config=nonccl \
    //native_client:libstt.so

# Copy built libs to /STT/native_client
RUN cp bazel-bin/native_client/libstt.so /STT/native_client/

# Build client.cc and install Python client and decoder bindings
ENV TFDIR /STT/tensorflow

RUN nproc

WORKDIR /STT/native_client
RUN make NUM_PROCESSES=$(nproc) stt

WORKDIR /STT
RUN cd native_client/python && make NUM_PROCESSES=$(nproc) bindings


# << END Build and bind


# Build KenLM
FROM nvcr.io/nvidia/tensorflow:20.06-tf1-py3 AS kenlm-build

ARG kenlm_repo=kpu/kenlm
ARG kenlm_branch=87e85e66c99ceff1fab2500a7c60c01da7315eec
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential cmake libboost-system-dev \
    libboost-thread-dev libboost-program-options-dev \
    libboost-test-dev libeigen3-dev zlib1g-dev \
    libbz2-dev liblzma-dev && \
    rm -rf /var/lib/apt/lists/*

# Build KenLM to generate new scorers
WORKDIR /code

RUN git clone https://github.com/${kenlm_repo} && \
    cd /code/kenlm && \
	git checkout $kenlm_branch && \
    mkdir -p build && \
    cd build && \
    cmake .. && \
    make -j $(nproc) || \
    ( echo "ERROR: Failed to build KenLM."; \
    echo "ERROR: Make sure you update the kenlm submodule on host before building this Dockerfile."; \
    echo "ERROR: $ cd STT; git submodule update --init kenlm"; \
    exit 1; )


# Final image for training
FROM nvidia/cuda:10.0-cudnn7-runtime-ubuntu18.04
#nvcr.io/nvidia/tensorflow:20.06-tf1-py3

# Import Args in Env
ARG cc_repo=mozilla/CorporaCreator
ARG cc_sha1=73622cf8399f8e634aee2f0e76dacc879226e3ac

# Model parameters
ARG model_language=fr
ENV MODEL_LANGUAGE=$model_language

# Training hyper-parameters
ARG batch_size=64
ENV BATCH_SIZE=$batch_size

ARG n_hidden=2048
ENV N_HIDDEN=$n_hidden

ARG epochs=30
ENV EPOCHS=$epochs

ARG learning_rate=0.0001
ENV LEARNING_RATE=$learning_rate

ARG dropout=0.3
ENV DROPOUT=$dropout

ARG lm_top_k=500000
ENV LM_TOP_K=500000

ARG lm_alpha=0.0
ENV LM_ALPHA=$lm_alpha

ARG lm_beta=0.0
ENV LM_BETA=$lm_beta

ARG beam_width=500
ENV BEAM_WIDTH=$beam_width

ARG early_stop=1
ENV EARLY_STOP=$early_stop

ARG amp=0
ENV AMP=$amp

# Dataset management
ARG duplicate_sentence_count=1
ENV DUPLICATE_SENTENCE_COUNT=$duplicate_sentence_count

# Should be of the form: lm_alpha_max,lm_beta_max,n_trials
ARG lm_evaluate_range=
ENV LM_EVALUATE_RANGE=$lm_evaluate_range

# Others
ARG english_compatible=0
ENV ENGLISH_COMPATIBLE=$english_compatible

ARG uid=999
ENV UID=$uid

ARG gid=999
ENV GID=$gid

# Make sure we can extract filenames with UTF-8 chars
ENV LANG=C.UTF-8

# Avoid keyboard-configuration step
ENV DEBIAN_FRONTEND noninteractive

ENV HOMEDIR /home/trainer

ENV VIRTUAL_ENV_NAME stt-train
ENV VIRTUAL_ENV $HOMEDIR/$VIRTUAL_ENV_NAME
ENV STT_DIR $HOMEDIR/stt
ENV KENLM_BIN $STT_DIR/kenlm/build/bin
ENV CC_DIR $HOMEDIR/cc

ENV PATH="$KENLM_BIN:${HOMEDIR}/.local/bin:${VIRTUAL_ENV}/bin:$PATH"

ENV LD_LIBRARY_PATH="$LD_LIBRARY_PATH:$STT_DIR:$KENLM_BIN:${STT_DIR}/data/lm"

RUN env

# We need to purge python3-xdg because
# it's breaking STT install later with
# errors about setuptools
#
RUN apt-get -qq update && \
    apt-get -qq install -y --no-install-recommends \
        build-essential \
        curl \
        wget \
        git \
        ffmpeg \
        python3 \
        python3-pip \
        ca-certificates \
        cmake \
        libboost-all-dev \
        zlib1g-dev \
        libbz2-dev \
        liblzma-dev \
        libmagic-dev \
        libopus0 \
        libopusfile0 \
        libsndfile1 \
        pkg-config \
        g++ \
        virtualenv \
        unzip \
        pixz \
        sox \
        sudo \
        libsox-fmt-all \
        locales locales-all \
        xz-utils && \
    apt-get purge -y python3-xdg && \
    rm -rf /var/lib/apt/lists/*

RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1

# Make sure pip and its dependencies are up-to-date
RUN pip3 install --upgrade pip wheel setuptools

# Creating user for security purposes
RUN groupadd -g $GID trainer && \
    adduser --system --uid $UID --group trainer

#RUN mkdir -p /etc/sudoers.d

RUN echo "trainer ALL=(root) NOPASSWD:ALL" > /etc/sudoers.d/trainer && \
    chmod 0440 /etc/sudoers.d/trainer

# Below that point, nothing requires being root
USER trainer

WORKDIR $HOMEDIR

RUN virtualenv --python=/usr/bin/python3 $VIRTUAL_ENV_NAME

# Copy files from previous build stages
COPY --chown=trainer:trainer --from=stt-build /STT ${STT_DIR}

WORKDIR $STT_DIR
# Build CTC decoder first, to avoid clashes on incompatible versions upgrades
RUN cd native_client/ctcdecode && make NUM_PROCESSES=$(nproc) bindings
RUN ${VIRTUAL_ENV}/bin/python -m pip install --upgrade native_client/ctcdecode/dist/*.whl

RUN mkdir -p ${STT_DIR}/kenlm/build/
COPY --chown=trainer:trainer --from=kenlm-build /code/kenlm/build/bin ${STT_DIR}/kenlm/build/bin

# Tool to convert output graph for inference
RUN curl -L https://github.com/coqui-ai/STT/releases/download/v0.9.3/convert_graphdef_memmapped_format.linux.amd64.zip | funzip > convert_graphdef_memmapped_format && \
    chmod +x convert_graphdef_memmapped_format

# Pre-built native client tools
RUN curl -L https://github.com/coqui-ai/STT/releases/download/v1.1.0/native_client.tflite.Linux.tar.xz | tar -xJvf - && rm ${STT_DIR}/libsox.so.*

# Install STT
# No need for the decoder since we did it earlier

RUN ${VIRTUAL_ENV}/bin/python -m pip install --upgrade native_client/python/dist/*.whl

RUN DS_NODECODER=y DS_NOTENSORFLOW=y python -m pip install --upgrade -e .

RUN ${VIRTUAL_ENV}/bin/python -m pip install --upgrade 'tensorflow-gpu==1.15.4'

# Install Corpus creator
WORKDIR $HOMEDIR

RUN git clone https://github.com/$cc_repo.git $CC_DIR

WORKDIR $CC_DIR

RUN git checkout $cc_sha1

WORKDIR $CC_DIR

# Copy copora patch
COPY --chown=trainer:trainer corpora.patch $CC_DIR

RUN patch -p1 < corpora.patch

# error: parso 0.7.0 is installed but parso<0.9.0,>=0.8.0 is required by {'jedi'}
RUN ${VIRTUAL_ENV}/bin/python -m pip install parso==0.8.3

RUN ${VIRTUAL_ENV}/bin/python -m pip install --upgrade pip~=21.3.1 wheel~=0.37.0 setuptools~=59.6.0

RUN ${VIRTUAL_ENV}/bin/python -m pip install modin[all]

RUN ${VIRTUAL_ENV}/bin/python setup.py install

# For CC PMF importer
RUN ${VIRTUAL_ENV}/bin/python -m pip install num2words zipfile38

# For MLS importer
RUN ${VIRTUAL_ENV}/bin/python -m pip install git+https://github.com/TeamPyOgg/PyOgg

WORKDIR ${HOMEDIR}

# Copy now so that docker build can leverage caches
COPY --chown=trainer:trainer . ${HOMEDIR}/

COPY --chown=trainer:trainer ${MODEL_LANGUAGE}/ ${HOMEDIR}/${MODEL_LANGUAGE}/

WORKDIR ${HOMEDIR}

ENTRYPOINT "$HOMEDIR/run.sh"